# Chat with CSV

A Streamlit application that allows users to upload CSV files and interact with the data using natural language.

<img width="1801" height="892" alt="Screenshot from 2025-12-28 19-26-50" src="https://github.com/user-attachments/assets/e602ada8-5cc2-4a9f-a1fa-7076b0e9dae5" />

## Overview

Chat with CSV leverages Large Language Models (LLMs) to enable natural language interaction with structured data. Users can upload a CSV file and ask questions in plain English. The system intelligently generates Python code to analyze the data, executes it in a secure sandbox, and returns the results or visualizations.

## Architecture

- **Frontend**: Streamlit-based user interface for file upload and chat interaction.
- **Agent Layer**: Utilizes OpenAI's GPT models to transform natural language into executable Python code.
- **Execution Engine**: A sandboxed environment that parses, validates, and executes generated code using AST analysis to ensure safety.

## Key Features

- Natural language querying of CSV data
- Streaming responses with intermediate tool execution feedback
- Secure sandbox for executing model-generated Python code
- Static code analysis using AST
- Restricted builtins and module allowlist
- Rate limiting to control API usage
- JSON structured logging
- Dockerized deployment
- Basic security-focused test coverage

## Security Model

Code generated by the model is never executed directly.

Before execution:
- Code is parsed into an AST and inspected for unsafe operations
- Only a small allowlist of Python modules is permitted
- Dangerous builtins such as open, exec, eval, and import hooks are blocked
- Execution runs with restricted globals and sanitized locals
- File system, OS, and subprocess access are disabled

## Prerequisites

- Python 3.10+
- Docker (optional)
- OpenAI API Key

## Setup

1. Clone the repository.
2. Copy `.env.example` to `.env` and add your `OPENAI_API_KEY`:
   ```bash
   cp .env.example .env
   ```

## Running with Docker

Build and run the container:

```bash
docker-compose up --build
```

Access the application at `http://localhost:8501`.

## Running Locally

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Run the application:
   ```bash
   streamlit run app/main.py
   ```
